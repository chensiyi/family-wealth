#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸“ä¸šé‡‘èæ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
åŸºäºç‚’è‚¡è½¯ä»¶æ•°æ®æ¨¡å‹è®¾è®¡
"""

import sqlite3
import json
import logging
from datetime import datetime, date
from typing import List, Dict, Optional
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('database_init.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ProfessionalFinanceDatabase:
    """ä¸“ä¸šé‡‘èæ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = 'professional_finance.db'):
        self.db_path = db_path
        self.connection = None
        self._connect()
        self._initialize_schema()
    
    def _connect(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        try:
            self.connection = sqlite3.connect(self.db_path)
            self.connection.row_factory = sqlite3.Row  # ä½¿ç»“æœå¯ä»¥é€šè¿‡åˆ—åè®¿é—®
            logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {self.db_path}")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    def _initialize_schema(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„...")
        
        # 1. å¸‚åœºæ•°æ®è¡¨
        self._create_market_data_table()
        
        # 2. å®è§‚ç»æµæŒ‡æ ‡è¡¨
        self._create_economic_indicators_table()
        
        # 3. èµ„äº§é…ç½®å†å²è¡¨
        self._create_asset_allocation_table()
        
        # 4. ç¨æ”¶æ”¿ç­–å†å²è¡¨
        self._create_tax_policy_table()
        
        # 5. å±æœºäº‹ä»¶åˆ†æè¡¨
        self._create_crisis_analysis_table()
        
        # 6. æŠ•èµ„è€…è¡Œä¸ºè¿½è¸ªè¡¨
        self._create_investor_behavior_table()
        
        # 7. æ•°æ®æºå…ƒä¿¡æ¯è¡¨
        self._create_data_source_table()
        
        # 8. åˆ›å»ºç´¢å¼•
        self._create_indexes()
        
        # 9. åˆ›å»ºè§†å›¾
        self._create_views()
        
        # 10. åˆ›å»ºè§¦å‘å™¨
        self._create_triggers()
        
        logger.info("âœ… æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")
    
    def _create_market_data_table(self):
        """åˆ›å»ºå¸‚åœºæ•°æ®è¡¨"""
        sql = """
        CREATE TABLE IF NOT EXISTS market_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            symbol TEXT NOT NULL,
            date DATE NOT NULL,
            open_price REAL,
            high_price REAL,
            low_price REAL,
            close_price REAL NOT NULL,
            volume INTEGER,
            adjusted_close REAL,
            dividend REAL DEFAULT 0,
            split_coefficient REAL DEFAULT 1,
            source TEXT NOT NULL,
            fetched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            data_quality_score REAL DEFAULT 1.0,
            UNIQUE(symbol, date)
        )
        """
        self.connection.execute(sql)
        logger.info("âœ… å¸‚åœºæ•°æ®è¡¨åˆ›å»ºå®Œæˆ")
    
    def _create_economic_indicators_table(self):
        """åˆ›å»ºå®è§‚ç»æµæŒ‡æ ‡è¡¨"""
        sql = """
        CREATE TABLE IF NOT EXISTS economic_indicators (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            indicator_name TEXT NOT NULL,
            country_code TEXT DEFAULT 'US',
            date DATE NOT NULL,
            value REAL NOT NULL,
            previous_value REAL,
            forecast_value REAL,
            unit TEXT,
            frequency TEXT CHECK(frequency IN ('daily', 'weekly', 'monthly', 'quarterly', 'yearly')),
            source TEXT NOT NULL,
            fetched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            reliability_score REAL DEFAULT 1.0,
            UNIQUE(indicator_name, country_code, date)
        )
        """
        self.connection.execute(sql)
        logger.info("âœ… å®è§‚ç»æµæŒ‡æ ‡è¡¨åˆ›å»ºå®Œæˆ")
    
    def _create_asset_allocation_table(self):
        """åˆ›å»ºèµ„äº§é…ç½®å†å²è¡¨"""
        sql = """
        CREATE TABLE IF NOT EXISTS asset_allocation_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            portfolio_id TEXT NOT NULL,
            date DATE NOT NULL,
            asset_class TEXT NOT NULL,
            allocation_percentage REAL NOT NULL,
            market_value REAL NOT NULL,
            cost_basis REAL,
            unrealized_gain_loss REAL,
            currency TEXT DEFAULT 'USD',
            rebalance_reason TEXT,
            strategy_reference TEXT,
            recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(portfolio_id, asset_class, date)
        )
        """
        self.connection.execute(sql)
        logger.info("âœ… èµ„äº§é…ç½®å†å²è¡¨åˆ›å»ºå®Œæˆ")
    
    def _create_tax_policy_table(self):
        """åˆ›å»ºç¨æ”¶æ”¿ç­–å†å²è¡¨"""
        sql = """
        CREATE TABLE IF NOT EXISTS tax_policy_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            policy_type TEXT NOT NULL,
            jurisdiction TEXT NOT NULL,
            effective_date DATE NOT NULL,
            expiration_date DATE,
            rate_percentage REAL,
            rate_type TEXT CHECK(rate_type IN ('flat', 'progressive', 'regressive')),
            exemption_amount REAL,
            deduction_limit REAL,
            policy_description TEXT,
            source_document TEXT,
            verified BOOLEAN DEFAULT FALSE,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(policy_type, jurisdiction, effective_date)
        )
        """
        self.connection.execute(sql)
        logger.info("âœ… ç¨æ”¶æ”¿ç­–å†å²è¡¨åˆ›å»ºå®Œæˆ")
    
    def _create_crisis_analysis_table(self):
        """åˆ›å»ºå±æœºäº‹ä»¶åˆ†æè¡¨"""
        sql = """
        CREATE TABLE IF NOT EXISTS crisis_event_analysis (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            event_id TEXT NOT NULL UNIQUE,
            event_name TEXT NOT NULL,
            event_date DATE NOT NULL,
            event_category TEXT CHECK(event_category IN ('financial', 'political', 'natural_disaster', 'pandemic', 'geopolitical')),
            severity_level INTEGER CHECK(severity_level BETWEEN 1 AND 10),
            affected_markets TEXT,
            trigger_symbols TEXT,
            market_reaction_data TEXT,
            duration_days INTEGER,
            recovery_period_days INTEGER,
            economic_impact_estimate REAL,
            data_sources TEXT,
            analysis_notes TEXT,
            verified BOOLEAN DEFAULT FALSE,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """
        self.connection.execute(sql)
        logger.info("âœ… å±æœºäº‹ä»¶åˆ†æè¡¨åˆ›å»ºå®Œæˆ")
    
    def _create_investor_behavior_table(self):
        """åˆ›å»ºæŠ•èµ„è€…è¡Œä¸ºè¿½è¸ªè¡¨"""
        sql = """
        CREATE TABLE IF NOT EXISTS investor_behavior_tracking (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            participant_id TEXT NOT NULL,
            decision_date DATE NOT NULL,
            action_type TEXT CHECK(action_type IN ('buy', 'sell', 'hold', 'hedge', 'diversify')),
            asset_symbol TEXT,
            quantity REAL,
            price_per_unit REAL,
            total_amount REAL,
            portfolio_percentage REAL,
            decision_rationale TEXT,
            market_conditions TEXT,
            risk_assessment TEXT,
            confidence_level INTEGER CHECK(confidence_level BETWEEN 1 AND 10),
            outcome_measured BOOLEAN DEFAULT FALSE,
            actual_return REAL,
            benchmark_comparison REAL,
            measured_at DATE,
            performance_notes TEXT,
            recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """
        self.connection.execute(sql)
        logger.info("âœ… æŠ•èµ„è€…è¡Œä¸ºè¿½è¸ªè¡¨åˆ›å»ºå®Œæˆ")
    
    def _create_data_source_table(self):
        """åˆ›å»ºæ•°æ®æºå…ƒä¿¡æ¯è¡¨"""
        sql = """
        CREATE TABLE IF NOT EXISTS data_source_metadata (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source_name TEXT NOT NULL UNIQUE,
            source_type TEXT CHECK(source_type IN ('official', 'financial', 'alternative', 'news')),
            base_url TEXT,
            api_endpoint TEXT,
            authentication_required BOOLEAN DEFAULT FALSE,
            auth_method TEXT,
            rate_limit INTEGER,
            data_format TEXT CHECK(data_format IN ('json', 'csv', 'xml', 'api')),
            last_accessed TIMESTAMP,
            success_rate REAL DEFAULT 1.0,
            average_response_time REAL,
            reliability_score REAL DEFAULT 1.0,
            supported_indicators TEXT,
            contact_info TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """
        self.connection.execute(sql)
        logger.info("âœ… æ•°æ®æºå…ƒä¿¡æ¯è¡¨åˆ›å»ºå®Œæˆ")
    
    def _create_indexes(self):
        """åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½"""
        indexes = [
            # å¸‚åœºæ•°æ®ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_market_symbol_date ON market_data(symbol, date)",
            "CREATE INDEX IF NOT EXISTS idx_market_date ON market_data(date)",
            "CREATE INDEX IF NOT EXISTS idx_market_source ON market_data(source)",
            
            # å®è§‚ç»æµæŒ‡æ ‡ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_econ_indicator_date ON economic_indicators(indicator_name, date)",
            "CREATE INDEX IF NOT EXISTS idx_econ_country_date ON economic_indicators(country_code, date)",
            
            # èµ„äº§é…ç½®ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_allocation_portfolio_date ON asset_allocation_history(portfolio_id, date)",
            "CREATE INDEX IF NOT EXISTS idx_allocation_asset_date ON asset_allocation_history(asset_class, date)",
            
            # ç¨æ”¶æ”¿ç­–ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_tax_jurisdiction_date ON tax_policy_history(jurisdiction, effective_date)",
            "CREATE INDEX IF NOT EXISTS idx_tax_type_date ON tax_policy_history(policy_type, effective_date)",
            
            # å±æœºäº‹ä»¶ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_crisis_date_severity ON crisis_event_analysis(event_date, severity_level)",
            "CREATE INDEX IF NOT EXISTS idx_crisis_category ON crisis_event_analysis(event_category)",
            
            # æŠ•èµ„è€…è¡Œä¸ºç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_behavior_participant_date ON investor_behavior_tracking(participant_id, decision_date)",
            "CREATE INDEX IF NOT EXISTS idx_behavior_action_date ON investor_behavior_tracking(action_type, decision_date)",
            "CREATE INDEX IF NOT EXISTS idx_behavior_symbol_date ON investor_behavior_tracking(asset_symbol, decision_date)"
        ]
        
        for index_sql in indexes:
            self.connection.execute(index_sql)
        
        logger.info("âœ… æ•°æ®åº“ç´¢å¼•åˆ›å»ºå®Œæˆ")
    
    def _create_views(self):
        """åˆ›å»ºå¸¸ç”¨æŸ¥è¯¢è§†å›¾"""
        views = [
            # ç»¼åˆè¡Œæƒ…è§†å›¾
            """
            CREATE VIEW IF NOT EXISTS market_overview_view AS
            SELECT 
                md.symbol,
                md.date,
                md.close_price,
                md.volume,
                md.adjusted_close,
                ei.indicator_name,
                ei.value as indicator_value,
                ROW_NUMBER() OVER (PARTITION BY md.symbol ORDER BY md.date DESC) as rn
            FROM market_data md
            LEFT JOIN economic_indicators ei ON md.date = ei.date
            WHERE md.date >= date('now', '-30 days')
            ORDER BY md.symbol, md.date DESC
            """,
            
            # èµ„äº§é…ç½®åˆ†æè§†å›¾
            """
            CREATE VIEW IF NOT EXISTS asset_allocation_analysis AS
            SELECT 
                aah.portfolio_id,
                aah.asset_class,
                aah.date,
                aah.allocation_percentage,
                aah.market_value,
                LAG(aah.allocation_percentage) OVER (
                    PARTITION BY aah.portfolio_id, aah.asset_class 
                    ORDER BY aah.date
                ) as previous_allocation,
                aah.allocation_percentage - LAG(aah.allocation_percentage) OVER (
                    PARTITION BY aah.portfolio_id, aah.asset_class 
                    ORDER BY aah.date
                ) as allocation_change,
                aah.rebalance_reason
            FROM asset_allocation_history aah
            WHERE aah.date >= date('now', '-1 year')
            """,
            
            # å±æœºå½±å“åˆ†æè§†å›¾
            """
            CREATE VIEW IF NOT EXISTS crisis_impact_analysis AS
            SELECT 
                cea.event_name,
                cea.event_date,
                cea.severity_level,
                cea.duration_days,
                cea.economic_impact_estimate,
                md.symbol,
                md.date,
                md.close_price,
                LAG(md.close_price, 5) OVER (
                    PARTITION BY md.symbol 
                    ORDER BY md.date
                ) as pre_crisis_price,
                ((md.close_price - LAG(md.close_price, 5) OVER (
                    PARTITION BY md.symbol 
                    ORDER BY md.date
                )) / LAG(md.close_price, 5) OVER (
                    PARTITION BY md.symbol 
                    ORDER BY md.date
                )) * 100 as price_change_percent
            FROM crisis_event_analysis cea
            JOIN market_data md ON md.date BETWEEN 
                date(cea.event_date) AND 
                date(cea.event_date, '+5 days')
            ORDER BY cea.event_date DESC, md.symbol
            """
        ]
        
        for view_sql in views:
            self.connection.execute(view_sql)
        
        logger.info("âœ… æ•°æ®åº“è§†å›¾åˆ›å»ºå®Œæˆ")
    
    def _create_triggers(self):
        """åˆ›å»ºè§¦å‘å™¨ç»´æŠ¤æ•°æ®ä¸€è‡´æ€§"""
        triggers = [
            # è‡ªåŠ¨æ›´æ–°æ—¶é—´æˆ³
            """
            CREATE TRIGGER IF NOT EXISTS update_market_data_timestamp 
            AFTER UPDATE ON market_data
            BEGIN
                UPDATE market_data SET fetched_at = CURRENT_TIMESTAMP WHERE id = NEW.id;
            END
            """,
            
            # æ•°æ®è´¨é‡è¯„åˆ†è‡ªåŠ¨è®¡ç®—
            """
            CREATE TRIGGER IF NOT EXISTS calculate_data_quality
            AFTER INSERT ON market_data
            BEGIN
                UPDATE market_data 
                SET data_quality_score = CASE 
                    WHEN NEW.source IN ('FRED', 'Yahoo Finance', 'SEC') THEN 0.95
                    WHEN NEW.source LIKE '%official%' THEN 0.9
                    ELSE 0.7
                END
                WHERE id = NEW.id;
            END
            """,
            
            # è‡ªåŠ¨æ›´æ–°æ•°æ®æºè®¿é—®æ—¶é—´
            """
            CREATE TRIGGER IF NOT EXISTS update_source_access_time
            AFTER INSERT ON market_data
            BEGIN
                UPDATE data_source_metadata 
                SET last_accessed = CURRENT_TIMESTAMP,
                    success_rate = (success_rate * 0.9 + 1.0 * 0.1)
                WHERE source_name = NEW.source;
            END
            """
        ]
        
        for trigger_sql in triggers:
            self.connection.execute(trigger_sql)
        
        logger.info("âœ… æ•°æ®åº“è§¦å‘å™¨åˆ›å»ºå®Œæˆ")
    
    def insert_sample_data(self):
        """æ’å…¥ç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•"""
        logger.info("ğŸ“Š å¼€å§‹æ’å…¥ç¤ºä¾‹æ•°æ®...")
        
        # 1. æ’å…¥æ•°æ®æºä¿¡æ¯
        data_sources = [
            ('FRED', 'official', 'https://fred.stlouisfed.org', '/api', False, 'api', 120, 'api', 0.98, 0.2, 'GDP,UNRATE,CPI'),
            ('Yahoo Finance', 'financial', 'https://finance.yahoo.com', '/v8/finance/chart', False, 'api', 2000, 'json', 0.95, 0.1, '^GSPC,AAPL,GOOGL'),
            ('SEC EDGAR', 'official', 'https://www.sec.gov', '/api', True, 'api_key', 10, 'json', 0.99, 0.5, '10-K,10-Q,13F')
        ]
        
        for source in data_sources:
            self.connection.execute("""
                INSERT OR IGNORE INTO data_source_metadata 
                (source_name, source_type, base_url, api_endpoint, authentication_required, 
                 auth_method, rate_limit, data_format, reliability_score, average_response_time, supported_indicators)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, source)
        
        # 2. æ’å…¥å®è§‚ç»æµæŒ‡æ ‡ç¤ºä¾‹æ•°æ®
        economic_data = [
            ('GDP', 'US', '2023-12-01', 22000.0, 21800.0, 22200.0, 'Billions USD', 'quarterly', 'FRED'),
            ('UNRATE', 'US', '2023-12-01', 3.7, 3.8, 3.6, 'Percent', 'monthly', 'FRED'),
            ('CPI', 'US', '2023-12-01', 315.0, 312.0, 318.0, 'Index', 'monthly', 'FRED')
        ]
        
        for data in economic_data:
            self.connection.execute("""
                INSERT OR IGNORE INTO economic_indicators 
                (indicator_name, country_code, date, value, previous_value, forecast_value, unit, frequency, source)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, data)
        
        # 3. æ’å…¥å¸‚åœºæ•°æ®ç¤ºä¾‹
        market_data_samples = [
            ('^GSPC', '2024-01-15', 4800.0, 4850.0, 4780.0, 4820.0, 3500000000, 4820.0, 0, 1, 'Yahoo Finance'),
            ('AAPL', '2024-01-15', 185.5, 187.2, 184.8, 186.3, 45000000, 186.3, 0, 1, 'Yahoo Finance'),
            ('GOOGL', '2024-01-15', 142.8, 144.5, 141.2, 143.7, 28000000, 143.7, 0, 1, 'Yahoo Finance')
        ]
        
        for data in market_data_samples:
            self.connection.execute("""
                INSERT OR IGNORE INTO market_data 
                (symbol, date, open_price, high_price, low_price, close_price, volume, adjusted_close, dividend, split_coefficient, source)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, data)
        
        # 4. æ’å…¥å±æœºäº‹ä»¶ç¤ºä¾‹æ•°æ®
        crisis_events = [
            ('2008_subprime_crisis', '2008å¹´æ¬¡è´·å±æœº', '2008-09-15', 'financial', 9, 
             '["US","EU","Global"]', '["^GSPC","BKLN","XLF"]', 
             '{"price_drop": -50, "volatility_spike": 80}', 365, 730, -2000.0,
             '["SEC filings","Fed records","News reports"]', 'é›·æ›¼å…„å¼Ÿç ´äº§å¼•å‘çš„å…¨çƒé‡‘èå±æœº')
        ]
        
        for event in crisis_events:
            self.connection.execute("""
                INSERT OR IGNORE INTO crisis_event_analysis 
                (event_id, event_name, event_date, event_category, severity_level, 
                 affected_markets, trigger_symbols, market_reaction_data, duration_days, 
                 recovery_period_days, economic_impact_estimate, data_sources, analysis_notes)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, event)
        
        self.connection.commit()
        logger.info("âœ… ç¤ºä¾‹æ•°æ®æ’å…¥å®Œæˆ")
    
    def get_database_stats(self) -> Dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        # è·å–å„è¡¨è®°å½•æ•°
        tables = ['market_data', 'economic_indicators', 'asset_allocation_history', 
                 'tax_policy_history', 'crisis_event_analysis', 'investor_behavior_tracking', 
                 'data_source_metadata']
        
        for table in tables:
            try:
                cursor = self.connection.execute(f"SELECT COUNT(*) as count FROM {table}")
                count = cursor.fetchone()['count']
                stats[table] = count
            except:
                stats[table] = 0
        
        # è·å–æ•°æ®åº“å¤§å°
        if os.path.exists(self.db_path):
            stats['database_size_mb'] = round(os.path.getsize(self.db_path) / (1024 * 1024), 2)
        
        return stats
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.connection:
            self.connection.close()
            logger.info("ğŸ”’ æ•°æ®åº“è¿æ¥å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆå§‹åŒ–æ•°æ®åº“
        db = ProfessionalFinanceDatabase('family_wealth_professional.db')
        
        # æ’å…¥ç¤ºä¾‹æ•°æ®
        db.insert_sample_data()
        
        # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
        stats = db.get_database_stats()
        logger.info("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
        for table, count in stats.items():
            if table != 'database_size_mb':
                logger.info(f"  {table}: {count} æ¡è®°å½•")
        logger.info(f"  æ•°æ®åº“å¤§å°: {stats.get('database_size_mb', 0)} MB")
        
        # æµ‹è¯•æŸ¥è¯¢
        logger.info("ğŸ” æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½...")
        
        # æµ‹è¯•å¸‚åœºæ•°æ®æŸ¥è¯¢
        cursor = db.connection.execute("""
            SELECT symbol, date, close_price, volume 
            FROM market_data 
            ORDER BY date DESC, symbol 
            LIMIT 5
        """)
        results = cursor.fetchall()
        logger.info("ğŸ“ˆ æœ€æ–°å¸‚åœºæ•°æ®:")
        for row in results:
            logger.info(f"  {row['symbol']} | {row['date']} | ${row['close_price']} | {row['volume']:,}")
        
        # æµ‹è¯•è§†å›¾æŸ¥è¯¢
        cursor = db.connection.execute("SELECT * FROM market_overview_view LIMIT 3")
        view_results = cursor.fetchall()
        logger.info("ğŸ‘ï¸ å¸‚åœºæ¦‚è§ˆè§†å›¾æµ‹è¯•:")
        for row in view_results:
            logger.info(f"  {row['symbol']} | {row['date']} | ${row['close_price']}")
        
        db.close()
        logger.info("ğŸ‰ æ•°æ®åº“åˆå§‹åŒ–å’Œæµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main()